# -*- coding: utf-8 -*-
"""audio_deep_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNGzLEPi6HZGlXthohHPWcFOfiBoogIA

##Set up the environment and download dataset
"""

!pip install soundata einops==0.7.0 datasets -qU

import soundata
import pandas as pd
import math, random
import torch
import torchaudio
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import torchvision
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
import tqdm

from IPython.display import Audio
from pathlib import Path
from torch.utils.data import DataLoader, Dataset, random_split
from PIL import Image
from torch import nn
from torch import Tensor
from torch.optim.lr_scheduler import StepLR
from torchaudio import transforms
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from datasets import load_dataset

dataset = soundata.initialize('urbansound8k')
dataset.download()                                                              #download the dataset
dataset.validate()                                                              #validate the dataset

#define parameters for building network and processing audio
n_embd = 1536
device = 'cuda' if torch.cuda.is_available() else 'cpu'
learning_rate = 4e-5
step_size = 20
gamma = 0.1
n_head = 6
n_layer = 6
n_class = 10
dropout = 0.2
epochs = 25
batch_size = 64
patch_width = 4                                                                 #should be a factor of n_timeframe
patch_height = 32                                                               #should be a factor of n_mels
n_mels = 128
n_fft = 2048
hop_length = 1024
std_sampling_rate = 44100
std_audio_duration = 4000
n_samples = std_sampling_rate * std_audio_duration
n_timeframe = math.ceil((n_samples - n_fft)/hop_length) + 1
n_channel = 2
example_clip = dataset.choice_clip()

n_timeframe = math.ceil(((std_sampling_rate * 4) - n_fft)/hop_length) + 1
n_timeframe
4/172

"""##Hugging Face Dataset"""

hf_dataset = load_dataset("danavery/urbansound8K")
hf_df = pd.DataFrame(hf_dataset["train"])

array = hf_dataset['train'][56]['audio']['array']
array.shape

first_audio_array = hf_df.loc[0].audio['array']
print(first_audio_array.shape)

# Create an audio player widget in the notebook that plays the loaded audio
from IPython.display import Audio

array_test = hf_dataset['train'][56]['audio']["array"]
sampling_rate = hf_dataset['train'][56]['audio']["sampling_rate"]
audio_player = Audio(data=array, rate=sampling_rate) # 'data' takes the numpy array of audio, 'rate' takes the sampling rate
audio_player  # Display the audio player widget in the notebook

"""##Inspect audio sample - Librosa"""

def parse_audio_file(audio_clip, n_mels, n_fft, hop_length):
  # audio_data, sample_rate = librosa.load(example_clip.audio_path, sr=None, mono=False)
  # n_channel = audio_data.shape[0]                                                                                             #C - no of channels
  # sgram = librosa.stft(audio_data, n_fft = n_fft, hop_length = hop_length)                                                    #C, N - C: no of channels, N: no of samples
  # magnitude_sgram = np.abs(sgram)                                                                                             #C, F, T - C: no of channels, F: no of hertz frequency components, T: no of time frames
  # mel_scale_sgram = librosa.feature.melspectrogram(S = magnitude_sgram, sr = sample_rate, n_mels = n_mels)                    #C, F, T - C: no of channels, F: no of mel frequency components, T: no of time frames
  # mel_filters = librosa.filters.mel(n_fft = n_fft, sr = sample_rate, n_mels = n_mels)                                         #F, T - F: no of mel frequency components, T: no of time frames
  # mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref = np.min)                                                          #C, F, T - C: no of channels, F: no of mel frequency components, T: no of time frames


  audio_data, sample_rate = librosa.load(example_clip.audio_path, sr = None, mono = False)
  n_channel = audio_data.shape[0]
  sgram = librosa.stft(audio_data, n_fft = n_fft, hop_length = hop_length, window='hann', center = False)
  magnitude_sgram = np.abs(sgram)
  mel_filters = librosa.filters.mel(n_fft = n_fft, sr = sample_rate, n_mels = n_mels)
  mel_scale_sgram = librosa.feature.melspectrogram(S = magnitude_sgram, sr = sample_rate, n_mels = n_mels)
  mel_scale_sgram_dB = librosa.amplitude_to_db(mel_scale_sgram, ref = np.min,  top_db = 80)

  fig_size = (10, 4)

  print(f"no of channels: {n_channel}")
  print(f"clip label: {example_clip.class_label}")
  print(f"sample rate: {sample_rate}Hz")

  display(Audio(example_clip.audio_path))

  #plot waveform
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      librosa.display.waveshow(y = audio_data[i], sr = sample_rate)
      plt.title(f'Channel {i+1} Waveform')
      plt.xlabel('Time (s)')
      plt.ylabel('Amplitude')
      plt.title(f'Channel {i+1} Waveform')
  plt.tight_layout()

  #plot frequency spectrum
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(magnitude_sgram[i]);
      plt.title('Spectrum');
      plt.xlabel('Frequency Bin');
      plt.ylabel('Amplitude');
      plt.title(f'Channel {i+1} Frequency Spectrum')
  plt.tight_layout()

  #plot frequency spectrum for the first time frame
  frame_no = 0
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(magnitude_sgram[i, :, frame_no]);
      plt.title('Spectrum');
      plt.xlabel('Frequency Bin');
      plt.ylabel('Amplitude');
      plt.title(f'Channel {i+1} Frequency Spectrum: Time Frame {frame_no + 1}')
  plt.tight_layout()

  #plot spectogram
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      img = librosa.display.specshow(magnitude_sgram[i], sr = sample_rate, y_axis = 'fft', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f')
      plt.title(f'Channel {i+1} Spectrogram')
  plt.tight_layout()

  #plot the mel filter banks
  plt.figure(figsize = fig_size)
  for i in range(n_mels):
      plt.plot(np.linspace(0, sample_rate/2, int(1 + n_fft//2)), mel_filters[i], label=f'Mel filter {i+1}')

  plt.xlabel('Frequency (Hz)')
  plt.ylabel('Amplitude')
  plt.title('Mel Filter Bank')
  plt.grid(True)
  plt.show()

  #plot mel spectogram
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      img = librosa.display.specshow(mel_scale_sgram[i], sr = sample_rate, y_axis = 'mel', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f')
      plt.title(f'Channel {i+1} Mel Spectrogram')
  plt.tight_layout()

  #plot mel spectogram (dB)
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      img = librosa.display.specshow(mel_scale_sgram_dB[i], sr = sample_rate, y_axis = 'mel', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f dB')
      plt.title(f'Channel {i+1} Mel Spectrogram (dB)')
  plt.tight_layout()

parse_audio_file(example_clip, n_mels = n_mels, n_fft = n_fft, hop_length = hop_length)

"""##Inspect audio sample - Pytorch"""

def parse_audio_file(audio_clip, n_mels, n_fft, hop_length):
  audio_data, sample_rate = torchaudio.load(example_clip.audio_path)
  sgram = torch.stft(audio_data, n_fft = n_fft, hop_length = hop_length, window = torch.hann_window(n_fft), return_complex = True, center = False)
  magnitude_sgram = np.abs(sgram)
  mel_scale_transform = transforms.MelSpectrogram(sample_rate = sample_rate, n_fft = n_fft, hop_length = hop_length, n_mels = n_mels)
  mel_scale_sgram = mel_scale_transform(audio_data)
  mel_filters = torchaudio.functional.melscale_fbanks(n_freqs = int(n_fft // 2 + 1), sample_rate = sample_rate, n_mels = n_mels, f_min = 0.0, f_max = sample_rate / 2.0)
  mel_filters = mel_filters.transpose(0, 1)
  amp_to_db = transforms.AmplitudeToDB(stype = "magnitude", top_db = 80)
  normalised_mel_scale_sgram = mel_scale_sgram / torch.min(mel_scale_sgram)
  mel_scale_sgram_dB = amp_to_db(normalised_mel_scale_sgram)

  fig_size = (10, 4)

  print(f"no of channels: {n_channel}")
  print(f"clip label: {example_clip.class_label}")
  print(f"sample rate: {sample_rate}Hz")

  display(Audio(example_clip.audio_path))

  #plot waveform of one timeframe
  audio_data_np = audio_data.numpy()
  duration = audio_data.size(1) / sample_rate
  time_axis = np.linspace(0, duration, audio_data.size(1))
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(time_axis, audio_data_np[i])
      plt.xlim(0, 0.02338)
      plt.title(f'Channel {i+1} Waveform: Timeframe 1')
      plt.xlabel('Time (s)')
      plt.ylabel('Amplitude')

  plt.tight_layout()

  #plot waveform of entire signal
  audio_data_np = audio_data.numpy()
  duration = audio_data.size(1) / sample_rate
  time_axis = np.linspace(0, duration, audio_data.size(1))
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(time_axis, audio_data_np[i])
      plt.title(f'Channel {i+1} Waveform')
      plt.xlabel('Time (s)')
      plt.ylabel('Amplitude')

  plt.tight_layout()

  #plot frequency spectrum
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(magnitude_sgram[i].numpy());
      plt.title('Spectrum');
      plt.xlabel('Frequency Bin');
      plt.ylabel('Amplitude');
      plt.title(f'Channel {i+1} Frequency Spectrum')
  plt.tight_layout()

  #plot frequency spectrum for the first time frame
  frame_no = 0
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      plt.plot(magnitude_sgram[i, :, frame_no].numpy());
      plt.title('Spectrum');
      plt.xlabel('Frequency Bin');
      plt.ylabel('Amplitude');
      plt.title(f'Channel {i+1} Frequency Spectrum: Time Frame {frame_no + 1}')
  plt.tight_layout()

  #plot the mel filter banks
  plt.figure(figsize = fig_size)
  for i in range(n_mels):
      plt.plot(np.linspace(0, sample_rate/2, int(1 + n_fft//2)), mel_filters[i].numpy(), label=f'Mel filter {i+1}')

  plt.xlabel('Frequency (Hz)')
  plt.ylabel('Amplitude')
  plt.xlim(0, 1000)
  plt.title('Mel Filter Bank')
  plt.grid(True)
  plt.show()

  #plot spectogram
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      # img = plt.imshow(magnitude_sgram[i].numpy(), aspect = 'auto', origin = 'lower', cmap = 'magma')
      img = librosa.display.specshow(magnitude_sgram[i].numpy(), sr = sample_rate, y_axis = 'fft', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f')
      plt.title(f'Channel {i+1} Spectrogram')
  plt.tight_layout()

  #plot mel spectogram
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      # img = plt.imshow(mel_scale_sgram[i].numpy(), aspect = 'auto', origin = 'lower', cmap = 'magma')
      img = librosa.display.specshow(mel_scale_sgram[i].numpy(), sr = sample_rate, y_axis = 'mel', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f')
      plt.title(f'Channel {i+1} Mel Spectrogram')
  plt.tight_layout()

  #plot mel spectogram (dB)
  plt.figure(figsize = fig_size)
  for i in range(n_channel):
      plt.subplot(1, n_channel, i+1)
      # img = plt.imshow(mel_scale_sgram_dB[i].numpy(), aspect = 'auto', origin = 'lower', cmap = 'magma')
      img = librosa.display.specshow(mel_scale_sgram_dB[i].numpy(), sr = sample_rate, y_axis = 'mel', x_axis = 'time')
      plt.colorbar(img, format='%+2.0f dB')
      plt.title(f'Channel {i+1} Mel Spectrogram (dB)')
  plt.tight_layout()

parse_audio_file(example_clip, n_mels = n_mels, n_fft = n_fft, hop_length = hop_length)

"""##Juxtaposing Pytorch and Librosa"""

print("-------------------------------Librosa-------------------------------")
audio_data, sample_rate = librosa.load(example_clip.audio_path, sr=None, mono=False)
sgram = librosa.stft(audio_data, n_fft = n_fft, hop_length = hop_length, window='hann', center = False)
magnitude_sgram = np.abs(sgram)
mel_filters = librosa.filters.mel(n_fft = n_fft, sr = sample_rate, n_mels = n_mels)
mel_scale_sgram = librosa.feature.melspectrogram(S = magnitude_sgram, sr = sample_rate, n_mels = n_mels)
mel_scale_sgram_dB = librosa.amplitude_to_db(mel_scale_sgram, ref = np.min,  top_db = 80)
print(f"audio_data shape:                     {audio_data.shape}")
print(f"chan 1, 5 samples:                    {audio_data[0][:5]}")
print(f"sgram shape:                          {sgram.shape}")
print(f"chan 1, freq 1, 5 time frames:        {sgram[0][0][:5]}")
print(f"magnitude_sgram shape:                {magnitude_sgram.shape}")
print(f"chan 1, freq 1, 5 time frames:        {magnitude_sgram[0][0][:5]}")
print(f"mel_filters shape:                    {mel_filters.shape}")
print(f"mel_scale_sgram shape:                {mel_scale_sgram.shape}")
print(f"mel_scale_sgram_dB shape:             {mel_scale_sgram_dB.shape}")
print(f"chan 1, freq 1, 5 time frames:        {mel_scale_sgram_dB[0][0][:5]}")

print("\n-------------------------------Pytorch-------------------------------")
audio_data, sample_rate = torchaudio.load(example_clip.audio_path)
sgram = torch.stft(audio_data, n_fft = n_fft, hop_length = hop_length, window = torch.hann_window(n_fft), return_complex = True, center = False)
magnitude_sgram = np.abs(sgram)
mel_scale_transform = transforms.MelSpectrogram(sample_rate = sample_rate, n_fft = n_fft, hop_length = hop_length, n_mels = n_mels)
mel_scale_sgram = mel_scale_transform(audio_data)
mel_filters = torchaudio.functional.melscale_fbanks(n_freqs = int(n_fft // 2 + 1), sample_rate = sample_rate, n_mels = n_mels, f_min = 0.0, f_max = sample_rate / 2.0)
amp_to_db = transforms.AmplitudeToDB(stype = "magnitude", top_db = 80)
mel_scale_sgram_dB = amp_to_db(mel_scale_sgram / torch.min(mel_scale_sgram))
mel_scale_sgram_dB_norm = (mel_scale_sgram_dB - mel_scale_sgram_dB.min()) / (mel_scale_sgram_dB.max() - mel_scale_sgram_dB.min())

print(f"audio_data shape:                     {audio_data.shape}")
print(f"chan 1, 5 samples:                    {audio_data[0][:5]}")
print(f"sgram shape:                          {sgram.shape}")
print(f"chan 1, freq 1, 5 time frames:        {sgram[0][0][:5]}")
print(f"magnitude_sgram shape:                {magnitude_sgram.shape}")
print(f"chan 1, freq 1, 5 time frames:        {magnitude_sgram[0][0][:5]}")
print(f"mel_filters shape:                    {mel_filters.shape}")
print(f"mel_scale_sgram shape:                {mel_scale_sgram.shape}")
print(f"mel_scale_sgram_dB shape:             {mel_scale_sgram_dB.shape}")
print(f"chan 1, freq 1, 5 time frames:        {mel_scale_sgram_dB[0][0][:5]}")
print(f"chan 1, freq 1, 5 time frames:        {mel_scale_sgram_dB_norm[0][0][:5]}")

"""##Utility functions for audio data preprocessing"""

class AudioUtil():
  #Load an audio file. Return the signal as a tensor and the sample rate
  @staticmethod
  def open(audio_file):
    sig, sr = torchaudio.load(audio_file)
    return (sig, sr)

  #Convert the given audio to the desired number of channels - mono or stereo
  @staticmethod
  def rechannel(aud, desired_no_of_channel):
    sig, sr = aud

    if (sig.shape[0] == desired_no_of_channel):                                                 #signal already has the desired no of channels
      return aud

    if (desired_no_of_channel == 1):                                                            #desired number of channels is 1
      resig = torch.mean(sig, dim = 0, keepdim = True)                                          #convert to 1 channel by averaging signal across both channels
      # resig = sig[:1, :]                                                                        #convert to 1 channel by selecting the first channel
    else:                                                                                       #desired number of channels is 2
      resig = torch.cat([sig, sig])                                                             #convert to 2 channels by duplicating the signal
    return ((resig, sr))

  #Standardize the sampling rate, so a second of recording holds the same number of samples
  @staticmethod
  def resample(aud, new_sampling_rate = std_sampling_rate):
    sig, sr = aud

    if (sr == new_sampling_rate):
      return aud

    # Resample the signal
    resampler = torchaudio.transforms.Resample(orig_freq = sr, new_freq = new_sampling_rate)    #define transform for resampling signal
    resig = resampler(sig)                                                                      #apply transform on the signal

    return (resig, new_sampling_rate)

  #Pad or truncate the signal to a fixed length 'max_ms' in milliseconds, so all samples have the same duration
  @staticmethod
  def pad_trunc(aud, max_ms = std_audio_duration):
    sig, sr = aud
    num_rows, sig_len = sig.shape
    max_len = sr//1000 * max_ms                                                 #convert the max time in milliseconds to max no. of samples needed

    if (sig_len > max_len):                                                     #if the signal is longer than the max length
      sig = sig[:,:max_len]                                                     #truncate the signal to the max length

    elif (sig_len < max_len):                                                   #if the signal is shorter than the max length
      pad_begin_len = random.randint(0, max_len - sig_len)                      #calculate the no. of samples to pad at the beginning
      pad_end_len = max_len - sig_len - pad_begin_len                           #calculate the no. of samples to pad at the end

      pad_begin = torch.zeros((num_rows, pad_begin_len))                        #create zeros tensor for beginning padding
      pad_end = torch.zeros((num_rows, pad_end_len))                            #create zeros tensor for end padding

      sig = torch.cat((pad_begin, sig, pad_end), dim = 1)                       #pad the signal with zeros at the beginning and end

    return (sig, sr)

  #Roll the signal to the left or right by some percent with wraparound
  @staticmethod
  def time_shift(aud, shift_limit):
    sig,sr = aud
    _, sig_len = sig.shape
    shift_amt = int(random.random() * shift_limit * sig_len)
    return (sig.roll(shift_amt), sr)

  # Generate a Mel Spectrogram
  @staticmethod
  def spectogram(aud, n_mels = n_mels, n_fft = n_fft, hop_length = hop_length):
    sig, sr = aud
    top_db = 80                                                                                                     #maximum no of decibels to be heard

    #create a transform
    mel_spectogram = transforms.MelSpectrogram(sr, n_fft = n_fft, hop_length = hop_length, n_mels = n_mels)         #instantiate a mel scale transformation class
    spec = mel_spectogram(sig)                                                                                      #apply transformation on the audio signal
    amp_to_db = transforms.AmplitudeToDB(top_db = top_db)                                                           #instantiate a dB scale transformation class
    spec = amp_to_db(spec)                                                                                          #apply transformation on the audio signal
    return (spec)

  @staticmethod
  def normalize_spectogram(spec):
    spec_min = spec.min()
    spec_max = spec.max()
    spec_norm = (spec - spec_min) / (spec_max - spec_min)
    return spec_norm

  #Mask specified % of frequency bins and time frames by populating them with the mean of the entire spectogram
  @staticmethod
  def augment_spectogram(spec, max_mask_pct = 0.1, n_freq_masks = 1, n_time_masks = 1):
    _, n_mels, n_steps = spec.shape
    mask_value = spec.mean()
    aug_spec = spec

    freq_mask_param = max_mask_pct * n_mels                                     #calculate the number of frequency bins that will be masked
    for _ in range(n_freq_masks):
      frequency_bin_mask = transforms.FrequencyMasking(freq_mask_param)
      aug_spec = frequency_bin_mask(aug_spec, mask_value)

    time_mask_param = max_mask_pct * n_steps                                    #calculate the number of time frames that will be masked
    for _ in range(n_time_masks):
      time_frame_mask = transforms.TimeMasking(time_mask_param)
      aug_spec = time_frame_mask(aug_spec, mask_value)

    return aug_spec

#Create a class from which we would mint a dataset object. This class would inherit from the Dataset library in torch.utils.data
#In this class, we would have methods that would pre-process our audio data using our staticmethod functions defined in AudioUtils class above

class SoundDS(Dataset):
  def __init__(self, df, n_mels = n_mels, n_fft = n_fft, hop_length = hop_length):
    self.df = df
    self.duration = std_audio_duration
    self.sr = std_sampling_rate
    self.channel = 2
    self.shift_pct = 0.4
    self.n_mels = n_mels
    self.n_fft = n_fft
    self.hop_length = hop_length

  def __len__(self):
    return len(self.df)

  def __getitem__(self, idx):
    audio_file = self.df.loc[idx, 'file_path']
    class_id = self.df.loc[idx, 'classID']
    class_label = self.df.loc[idx, 'class']

    aud = AudioUtil.open(audio_file)
    reaud = AudioUtil.resample(aud, self.sr)
    rechan = AudioUtil.rechannel(reaud, self.channel)

    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)
    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)
    sgram = AudioUtil.spectogram(shift_aud, n_mels = self.n_mels, n_fft = self.n_fft, hop_length = self.hop_length)
    aug_sgram = AudioUtil.augment_spectogram(sgram, max_mask_pct = 0.1, n_freq_masks = 2, n_time_masks = 2)

    return aug_sgram, class_id

df = pd.read_csv('/root/sound_datasets/urbansound8k/metadata/UrbanSound8K.csv')
df['file_path'] = '/root/sound_datasets/urbansound8k/audio/fold' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str)
sound_dataset = SoundDS(df)

#Split dataset into training set (80%). test set (10%) and validation set (10%)
num_items = len(sound_dataset)
num_train_set = round(num_items * 0.8)
num_test_set = round(num_items * 0.1)
num_validation_set = num_items - num_train_set - num_test_set

train_split, test_split, validation_split = random_split(sound_dataset, [num_train_set, num_test_set, num_validation_set])

train_dataloader = DataLoader(train_split, batch_size = batch_size, shuffle = True)
test_dataloader = DataLoader(test_split, batch_size = batch_size, shuffle = True)
validation_dataloader = DataLoader(validation_split, batch_size = batch_size, shuffle = True)

print(f"No. of training samples:    {len(train_split)}")
print(f"No. of test samples:        {len(test_split)}")
print(f"No. of validation samples:  {len(validation_split)}")

"""##Define Transformer"""

class PatchEmbedding(nn.Module):
    def __init__(self, n_channel, patch_height, patch_width, n_embd):
      super().__init__()
      self.projection = nn.Sequential(
          Rearrange('batch channel (h s1) (w s2) -> batch (h w) (s1 s2 channel) ', s1 = patch_height, s2 = patch_width),                #patchify and flattening
          nn.Linear(patch_height * patch_width * n_channel, n_embd)                                                                     #embedding matrix
      )

    def forward(self, x: Tensor) -> Tensor:
      x = self.projection(x)
      return x

# Create class used to calculate self attention for a single attention head
class Head(nn.Module):

  """one head of self-attention"""
  def __init__(self, head_size):
    super().__init__()
    self.key = nn.Linear(n_embd, head_size, bias = False)                                   #generate a key matrix for each token in all the batches
    self.query = nn.Linear(n_embd, head_size, bias = False)                                 #generate a query matrix for each token in all the batches
    self.value = nn.Linear(n_embd, head_size, bias = False)                                 #generate a value matrix for each token in all the batches
    # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

  def forward(self, x):

    _, _, d = x.shape                                                                       #batch size, number of tokens, embedding dimension
    k = self.key(x)   # (B, T, C)
    q = self.query(x) # (B, T, C)
    v = self.value(x) # (B, T, C)

    # compute attention scores
    wei =  q @ k.transpose(-2, -1) * d**-0.5                                                 #for each batch: multiply keys by the queries (b, n, d) @ (B, d, n) ---> (b, d, d)
    # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))                             #comment out to enable bidirectional token interaction
    wei = F.softmax(wei, dim=-1)
    out = wei @ v
    return out

# Create class used to calculate a list of single attention heads
class MultiHeadAttention(nn.Module):

  """multiple heads of self-attention"""
  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(n_embd, n_embd)

  def forward(self, x):
    out = torch.cat([h(x) for h in self.heads], dim = -1)                                    #concatenate results of attention heads
    out = self.proj(out)                                                                     #projection back into residual pathway
    return out

class FeedForward(nn.Module):
  """ a simple linear layer followed by a non-linearity """
# This layer allows each token to "think" about the information gotten in the previous attention layer

  def __init__(self, n_embd):
      super().__init__()
      self.net = nn.Sequential(
          nn.Linear(n_embd, 4 * n_embd),
          nn.GELU(),
          nn.Dropout(dropout),
          nn.Linear(4 * n_embd, n_embd),
          nn.Dropout(dropout),
      )

  def forward(self, x):
      return self.net(x)

class TransformerBlock(nn.Module):
  """ Transformer block: communication followed by computation """

  def __init__(self, n_embd, n_head):
      # n_embd: embedding dimension, n_head: the number of heads we'd like
      super().__init__()
      head_size = n_embd // n_head
      self.sa = MultiHeadAttention(n_head, head_size)
      self.ffwd = FeedForward(n_embd)
      self.ln1 = nn.LayerNorm(n_embd)
      self.ln2 = nn.LayerNorm(n_embd)

  def forward(self, x):
      x = x + self.sa(self.ln1(x))
      x = x + self.ffwd(self.ln2(x))
      return x

class ClassificationHead(nn.Module):
  """ classifier head """
  def __init__(self, n_embd, n_class):
    super().__init__()
    self.cls_head = nn.Linear(n_embd, n_class)
    self.dropout = nn.Dropout(dropout)
    self.ln_f = nn.LayerNorm(n_class)

  def forward(self, x):
    x = self.dropout(x)
    x = self.cls_head(x[:, 0, :])                                               #pluck the first token from each of the batch
    x = self.ln_f(x)                                                            #layer normalization
    return x

class AudioClassifier(nn.Module):
  def __init__(self):
    super().__init__()

    n_patch = (n_mels // patch_height) * (n_timeframe // patch_width)
    self.patch_embed_master = PatchEmbedding(n_channel, patch_height, patch_width, n_embd)      #patchify, flatten, embed
    self.cls_token = nn.Parameter(torch.randn(1, 1, n_embd))                                    #classification token
    self.pos_embed = nn.Parameter(torch.randn(1, n_patch + 1, n_embd))                          #position embedding
    self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_layer)])
    self.dropout = nn.Dropout(dropout)
    self.class_head = ClassificationHead(n_embd, n_class)

  def forward(self, x):
    x = self.patch_embed_master(x)
    b, n, _ = x.shape

    cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)                                         #create classification token corresponding to no of inputs in each batch
    x = torch.cat([cls_tokens, x], dim = 1)                                                              #prepend classification token to at the start of the sequence for each input in the batch
    x += self.pos_embed[:, :(n + 1), :]                                                                  #add position embedding to each token in each sequence in the batch
    x = self.dropout(x)                                                                                  #add dropout after adding positional embedding like the ViT paper

    x = self.blocks(x)
    x = self.class_head(x)

    return x

"""##Training time"""

def train_epoch(
    model,
    epochs = epochs,
    learning_rate = learning_rate,
    n_class = n_class,
    classification_head = None,
    min_loss_improvement = 0.005):


  #Process classification head
  if classification_head is not None:                                           #classification head is provided
      for param in model.parameters():
          param.requires_grad = False                                           #freeze ALL mode parameters by disabling gradient computation

      model.class_head = classification_head                                    #replace the model classification head with the one provided

      for param in model.class_head.parameters():
          param.requires_grad = True                                            #enable gradient computation for the new classification head

  else:                                                                         #classification head is not provided, so we initialize one using the ClassificationHead class
    model.class_head = ClassificationHead(n_embd, n_class)


  optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)         #define Adam optimizer for use in loss minimization
  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)     #reduce learning rate by a factor of 0.1 every 7 epochs

  print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')
  print("--------------------TRAINING THE MODEL----------------------")
  print(device)

  model.to(device)
  model.train()
  iteration = 0
  training_loss_list = []
  training_accuracy_list = []
  test_loss_list = []
  test_accuracy_list = []
  previous_epoch_training_loss = float('inf')

  for epoch in range(epochs):
    epoch_training_loss = 0
    epoch_training_accuracy = 0
    for x, y in tqdm.tqdm(train_dataloader):
      x = x.to(device)
      y = y.to(device)

      y_hat = model(x)

      loss = F.cross_entropy(y_hat, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      accuracy = (y_hat.argmax(dim = 1) == y).float().mean()                    #get average prediction accuracy over the batch
      epoch_training_accuracy += accuracy
      epoch_training_loss += loss.item()

    epoch_training_accuracy /= len(train_dataloader)
    epoch_training_loss /= len(train_dataloader)

    training_loss_list.append(epoch_training_loss)
    training_accuracy_list.append(epoch_training_accuracy.cpu().numpy())
    # training_accuracy_list.append(epoch_training_accuracy.numpy())

    #Evaluate test loss
    model.eval()
    with torch.no_grad():
      epoch_test_loss = 0
      epoch_test_accuracy = 0
      for x, y in tqdm.tqdm(test_dataloader):
        x = x.to(device)
        y = y.to(device)

        y_hat = model(x)

        loss = F.cross_entropy(y_hat, y)

        accuracy = (y_hat.argmax(dim = 1) == y).float().mean()                  #get average prediction accuracy over the batch
        epoch_test_accuracy += accuracy
        epoch_test_loss += loss.item()

      epoch_test_accuracy /= len(test_dataloader)
      epoch_test_loss /= len(test_dataloader)
      test_loss_list.append(epoch_test_loss)
      test_accuracy_list.append(epoch_test_accuracy.cpu().numpy())


    model.train()
    scheduler.step()
    print(f"\nepoch: {epoch+1}\ntraining loss: {epoch_training_loss:.4f}\ntraining accuracy: {epoch_training_accuracy:.4f}\ntest loss: {epoch_test_loss:.4f}\ntest accuracy: {epoch_test_accuracy:.4f}\n")

    loss_improvement = previous_epoch_training_loss - epoch_training_loss
    if loss_improvement < min_loss_improvement:
      print("\nTraining halted due insufficient loss improvement")
      break
    previous_epoch_training_loss = epoch_training_loss

  #Plot loss
  plt.plot(training_loss_list, label = 'training loss')
  plt.plot(test_loss_list, label = 'test loss')
  plt.legend()
  plt.show()

  #Plot accuracy
  plt.plot(training_accuracy_list, label = 'training accuracy')
  plt.plot(test_accuracy_list, label = 'test accuracy')
  plt.legend()
  plt.show()

  return model
  # if save_head:
  #   return model.mlp_head.state_dict()                                          #return the saved head weights for potential re-use

model = AudioClassifier()
print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')

#------------------TEST UNTRAINED MODEL USING TEST DATASET---------------------------#
x_test, y_test = next(iter(test_dataloader))
y_hat_test = model(x_test)

print(f"shape of input is:  {x_test.shape}")
print(f"shape of predicted output is: {y_hat_test.shape}")
print(f"shape of actual output is: {y_test.shape}")
print("\n")
print("Predicted classes:", y_hat_test.argmax(-1))
print("Actual classes:", y_test)

model = train_epoch(
    model = model,
    epochs = epochs,
    learning_rate = learning_rate,
    n_class = n_class,
    classification_head = None,
    min_loss_improvement = -float('inf'))

#------------------TEST TRAINED MODEL USING TEST DATASET---------------------------#
x_test = x_test.to(device)
y_hat_test = model(x_test)

print(f"shape of input is:  {x_test.shape}")
print(f"shape of predicted output is: {y_hat_test.shape}")
print(f"shape of actual output is: {y_test.shape}")
print("\n")
print("Predicted classes:", y_hat_test.argmax(-1))
print("Actual classes:", y_test)